{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nukimayasari/scrapinf?scriptVersionId=219819635\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:46:59.716013Z","iopub.execute_input":"2025-01-29T17:46:59.716368Z","iopub.status.idle":"2025-01-29T17:47:00.207752Z","shell.execute_reply.started":"2025-01-29T17:46:59.716328Z","shell.execute_reply":"2025-01-29T17:47:00.206551Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Scraping and Translating Indonesian Soto Recipes from Cookpad\n\n## Introduction\n\n**Soto**, an iconic Indonesian soup dish, is beloved for its flavorful broth and unique combinations of ingredients. Recipes for Soto vary across regions, with each version offering a distinct taste and story. With abundant Soto recipes available online, wouldn't it be exciting to scrape and analyze these recipes to uncover patterns and insights?\n\nIn this notebook, we will explore how to scrape Soto recipes from Cookpad, a popular recipe-sharing platform, translate them into English, and extract structured information for further analysis. This step-by-step guide combines web scraping, data cleaning, and translation, making it an excellent learning resource for beginners and enthusiasts. This project is developed with the assistance of DeepSeek and ChatGPT.\n\n## Objectives\n* **Scrape Recipes:** Collect Soto recipe data, including names, ingredient lists, and URLs, from Cookpad.\n* **Extract Details:** Parse and structure the scraped information into a DataFrame.\n* Translate Ingredients:** Use a translation API to translate Indonesian ingredient names into English.\n* **Create a Dataset:** Save the processed data into a CSV file for easy sharing and future analysis.\n\n## Tools and Libraries\nWe will use the following Python libraries:\n* **`requests`**: To fetch HTML content from web pages.\n* **`BeautifulSoup`**: For parsing and extracting structured data from the HTML.\n* **`pandas`**: To organize and manipulate data in tabular form.\n* **`deep-translator`**: For translating text using Google Translate.\n\nBefore running the code, make sure you have installed these libraries:","metadata":{}},{"cell_type":"code","source":"!pip install requests beautifulsoup4 pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:47:00.209354Z","iopub.execute_input":"2025-01-29T17:47:00.210112Z","iopub.status.idle":"2025-01-29T17:47:05.563744Z","shell.execute_reply.started":"2025-01-29T17:47:00.210068Z","shell.execute_reply":"2025-01-29T17:47:05.562648Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:47:05.566237Z","iopub.execute_input":"2025-01-29T17:47:05.566557Z","iopub.status.idle":"2025-01-29T17:47:06.010751Z","shell.execute_reply.started":"2025-01-29T17:47:05.566529Z","shell.execute_reply":"2025-01-29T17:47:06.00959Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Steps\n\n### Step 1: Scraping Cookpad Recipes\n\nWe start by building a **scraper** to collect Soto recipes from Cookpad. The scraper navigates through multiple pages, fetching the recipe names and URLs.","metadata":{}},{"cell_type":"code","source":"def scrape_cookpad(query, num_pages=5):\n    base_url = \"https://cookpad.com/id/cari/\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n    }\n    recipes = []\n    \n    for page in range(1, num_pages + 1):\n        url = f\"{base_url}{query}?page={page}\"\n        response = requests.get(url, headers=headers)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Debugging: Print the soup to check structure\n        # print(soup.prettify())\n        \n        for recipe in soup.find_all('a', class_='block-link__main'):  # Update class here if needed\n            recipe_name = recipe.text.strip()\n            recipe_url = \"https://cookpad.com\" + recipe['href']\n            recipes.append((recipe_name, recipe_url))\n    \n    return recipes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:47:06.012239Z","iopub.execute_input":"2025-01-29T17:47:06.012886Z","iopub.status.idle":"2025-01-29T17:47:06.018655Z","shell.execute_reply.started":"2025-01-29T17:47:06.012855Z","shell.execute_reply":"2025-01-29T17:47:06.017666Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Step 2: Extracting Recipe Details\n\nNext, we fetch individual recipe pages to extract the recipe title and list of ingredients. Each recipe will be stored in a structured format.","metadata":{}},{"cell_type":"code","source":"def get_recipe_details(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Extract ingredients\n    ingredients = []\n    ingredient_section = soup.find('div', class_='ingredient-list')  # Update based on actual structure\n    if ingredient_section:\n        for li in ingredient_section.find_all('li'):\n            ingredients.append(li.text.strip())\n    \n    # Extract recipe title (Soto)\n    title_element = soup.find('h1')  # Try without specifying class first\n    recipe_title = title_element.text.strip() if title_element else \"Unknown\"\n    \n    return {\n        'Soto': recipe_title,\n        'Ingredients': ingredients,\n        'Link Source': url\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:47:06.01952Z","iopub.execute_input":"2025-01-29T17:47:06.019838Z","iopub.status.idle":"2025-01-29T17:47:06.038778Z","shell.execute_reply.started":"2025-01-29T17:47:06.019814Z","shell.execute_reply":"2025-01-29T17:47:06.037577Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Scrape Soto recipes\nsoto_recipes = scrape_cookpad(\"soto\")\n\n# Extract details for all recipes\nrecipe_details = []\nfor name, url in soto_recipes:\n    try:\n        details = get_recipe_details(url)\n        recipe_details.append(details)\n    except Exception as e:\n        print(f\"Error scraping {url}: {e}\")\n\n# Create a DataFrame\ndf = pd.DataFrame(recipe_details)\n\n# Display the DataFrame\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:47:06.039903Z","iopub.execute_input":"2025-01-29T17:47:06.040267Z","iopub.status.idle":"2025-01-29T17:48:03.212864Z","shell.execute_reply.started":"2025-01-29T17:47:06.040227Z","shell.execute_reply":"2025-01-29T17:48:03.211688Z"}},"outputs":[{"name":"stdout","text":"                               Soto  \\\n0  Soto Ayam Rumahan /Soto Lamongan   \n1       Rujak soto Bwi (Banyuwangi)   \n2                    Soto Mie Bogor   \n3                  Soto Ayam Santan   \n4      Soto Betawi kuah santan susu   \n\n                                         Ingredients  \\\n0  [Bahan utama:, 1 Kg dada ayam, 1/4 kg telur, 1...   \n1  [1/4 kg babat sapi dibersihkan sampai putih de...   \n2  [500 gr daging sapi (saya pilih sandung lamur)...   \n3  [1/2 kg ayam (aku pakai paha), 10 btr bwg mera...   \n4  [700 gr daging, 2 buah wortel, 2 buah kentang,...   \n\n                                         Link Source  \n0  https://cookpad.com/id/resep/24445055-soto-aya...  \n1  https://cookpad.com/id/resep/24444549-rujak-so...  \n2  https://cookpad.com/id/resep/24441258-soto-mie...  \n3  https://cookpad.com/id/resep/24441246-soto-aya...  \n4  https://cookpad.com/id/resep/24437089-soto-bet...  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:48:03.214139Z","iopub.execute_input":"2025-01-29T17:48:03.214542Z","iopub.status.idle":"2025-01-29T17:48:03.222978Z","shell.execute_reply.started":"2025-01-29T17:48:03.214502Z","shell.execute_reply":"2025-01-29T17:48:03.221939Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(150, 3)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### Step 3: Translating Ingredients\n\nMost of the ingredients are in Indonesian, so we use **deep-translator** to translate them into English. To speed up the process, we implement a translation **cache** to avoid redundant API calls.","metadata":{}},{"cell_type":"code","source":"!pip install deep-translator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:52:22.998673Z","iopub.execute_input":"2025-01-29T17:52:22.999101Z","iopub.status.idle":"2025-01-29T17:52:27.51522Z","shell.execute_reply.started":"2025-01-29T17:52:22.999067Z","shell.execute_reply":"2025-01-29T17:52:27.513952Z"}},"outputs":[{"name":"stdout","text":"Collecting deep-translator\n  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.12.14)\nDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: deep-translator\nSuccessfully installed deep-translator-1.11.4\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from deep_translator import GoogleTranslator\n\n# Initialize a translation cache\ntranslation_cache = {}\n\ndef translate_with_cache(ingredient):\n    \"\"\"\n    Translate a single ingredient using GoogleTranslator with caching.\n    \"\"\"\n    # Check if the ingredient is already in the cache\n    if ingredient in translation_cache:\n        return translation_cache[ingredient]\n    try:\n        # Translate the ingredient and store it in the cache\n        translated = GoogleTranslator(source='id', target='en').translate(ingredient)\n        translation_cache[ingredient] = translated\n        return translated\n    except Exception as e:\n        print(f\"Error translating ingredient: {ingredient}. Error: {e}\")\n        # Return the original ingredient if translation fails\n        return ingredient\n\ndef translate_recipe(recipe):\n    \"\"\"\n    Translate all ingredients in a recipe using the translation cache.\n    \"\"\"\n    recipe_translated = [translate_with_cache(ingredient) for ingredient in recipe]\n    return recipe_translated\n\n# Rename the current \"Ingredients\" column to \"Recipe\"\ndf.rename(columns={\"Ingredients\": \"Recipe\"}, inplace=True)\n\n# Create a new column \"Recipe_English\" with translated ingredients\ndf['Recipe_English'] = df['Recipe'].apply(translate_recipe)\n\n# Display the updated DataFrame\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:06:22.548387Z","iopub.execute_input":"2025-01-29T18:06:22.548783Z","iopub.status.idle":"2025-01-29T18:35:49.322861Z","shell.execute_reply.started":"2025-01-29T18:06:22.548751Z","shell.execute_reply":"2025-01-29T18:35:49.321743Z"}},"outputs":[{"name":"stdout","text":"                               Soto  \\\n0  Soto Ayam Rumahan /Soto Lamongan   \n1       Rujak soto Bwi (Banyuwangi)   \n2                    Soto Mie Bogor   \n3                  Soto Ayam Santan   \n4      Soto Betawi kuah santan susu   \n\n                                              Recipe  \\\n0  [Bahan utama:, 1 Kg dada ayam, 1/4 kg telur, 1...   \n1  [1/4 kg babat sapi dibersihkan sampai putih de...   \n2  [500 gr daging sapi (saya pilih sandung lamur)...   \n3  [1/2 kg ayam (aku pakai paha), 10 btr bwg mera...   \n4  [700 gr daging, 2 buah wortel, 2 buah kentang,...   \n\n                                         Link Source  \\\n0  https://cookpad.com/id/resep/24445055-soto-aya...   \n1  https://cookpad.com/id/resep/24444549-rujak-so...   \n2  https://cookpad.com/id/resep/24441258-soto-mie...   \n3  https://cookpad.com/id/resep/24441246-soto-aya...   \n4  https://cookpad.com/id/resep/24437089-soto-bet...   \n\n                                      Recipe_English  \n0  [The main ingredient:, 1 Kg chicken breast, 1/...  \n1  [1/4 kg of beef tripe, cleaned until white by ...  \n2  [500 gr beef (I chose brisket), 1 piece of ric...  \n3  [1/2 kg chicken (I used thigh), 10 pcs red bwg...  \n4  [700 gr meat, 2 carrots, 2 potatoes, 1 tomato,...  \n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Step 4: Saving the Dataset\n\nFinally, we save the processed data to a **CSV file** for further analysis or sharing.","metadata":{}},{"cell_type":"code","source":"df.to_csv(\"Soto_Recipe_Cookpad\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:05:12.632599Z","iopub.execute_input":"2025-01-29T19:05:12.633115Z","iopub.status.idle":"2025-01-29T19:05:12.656754Z","shell.execute_reply.started":"2025-01-29T19:05:12.633073Z","shell.execute_reply":"2025-01-29T19:05:12.655711Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Results\n\nThe resulting DataFrame contains the following columns:\n* **Soto:** Recipe title\n* **Recipe:** Original ingredient list in Indonesian\n* **Link Source:** URL to the original recipe on Cookpad\n* **Recipe_English:** Translated ingredient list in English","metadata":{}},{"cell_type":"markdown","source":"## Key Takeaways\n\n* This project demonstrates how to combine web scraping and translation APIs to create a structured dataset from unstructured web data.\n* You can adapt this workflow to scrape and analyze recipes for other dishes or from other platforms.\n* The final dataset can be used for exploratory data analysis, ingredient frequency analysis, or even building recommendation systems for recipe enthusiasts.","metadata":{}}]}